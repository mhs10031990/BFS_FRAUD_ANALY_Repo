{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f1244d-9cdf-4e8f-b313-c662936f68cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install fosforml numpy pandas matplotlib scikit-learn seaborn python-dateutil\n",
    "!pip uninstall urllib3 -y\n",
    "!pip install urllib3==1.26.15\n",
    "!pip install fosforml \n",
    "!pip install fosforio\n",
    "!pip install -U cloudpickle\n",
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae08a6c6-b883-46b5-b83a-4097a57c5294",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install seaborn scipy xgboost pandas dice-ml tabulate numpy scikit-learn pandas-profiling plotly matplotlib scipy statsmodels seaborn pydantic-settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5934fab0-0f2d-4684-a42e-e32204fc1613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from scipy.stats.mstats import winsorize\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "from fosforml import *\n",
    "from fosforml.constants import MLModelFlavours\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388a681d-9cfc-4925-97a8-d13bb7d93794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fosforml.model_manager.snowflakesession import get_session\n",
    "\n",
    "my_session = get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c76364-7064-4480-b6cc-7a5075125dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = 'FRAUD_DETECTION_ANALY_MASTER_TABLE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef45048-2b00-4a6f-92ab-f22ca6bd8fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_df = my_session.sql(\"select * from {}\".format(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b44499-f4a9-485a-bcef-39598314e338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pandas_df = sf_df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a2c9a3-6012-410b-a868-deefaf550300",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pandas_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314312fe-e702-4d9d-ba1d-078028152246",
   "metadata": {},
   "outputs": [],
   "source": [
    "Original_df = pandas_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acafe521-0819-4416-8c75-f6056856a38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a050a8-6ce6-4579-907e-1f0035f3ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Original_df.drop([\"TRANSACTION_ID\", \"CUSTOMER_ID\", \"DOB\", \"NAME\", \"CITY\",\"COUNTY\",\"STATE\",\"YEAR\",\"QUARTER\",\"MONTH\",\"AGE_GROUP\",\"TRANSACTION_DATE\",\"ACCOUNT_OPENING_DATE\",\"LAST_LOGIN\",\"TIME_OF_DAY\",\"DAY_OF_WEEK\",\"TIME\",\"TIME_SINCE_LAST_TRANSACTION\",\"FRAUD_FLAG\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0a145a-b5a7-4422-855b-9025ea839e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = [\"MERCHANT_CATEGORY\",\"PAYMENT_METHOD\",\"CUSTOMER_SEGMENT\",\"ACCOUNT_TYPE\",\"DEVICE_USED\",\"TRANSACTION_STATUS\",\"CROSS_BORDER_TRANSACTION_INDICATOR\",\"NEW_DEVICE_INDICATOR\",\"GENDER\",\"ANAMOLY_RISK_CATEGORY\",\"MERCHANT_RISK_CATEGORY\",\"CUSTOMER_LOYALTY_CATEGORY\",\"TRANSACTION_TYPE\"]\n",
    "NUMERICAL_COLUMNS = [\"ANOMALY_SCORE\",\"TRANSACTION_AMOUNT\",\"CREDIT_LIMIT\",\"AGE_OF_ACCOUNT\",\"FREQUENCY_OF_TRANSACTIONS\",\"MERCHANT_RISK_SCORE\",\"CUSTOMER_LOYALTY_SCORE\",\"DISTANCE_FROM_HOME_ADDRESS\",\"DISTANCE_FROM_LAST_TRANSACTION\",\"NUMBER_OF_REFUNDS\",\"NUMBER_OF_CHARGEBACKS\",\"CREDIT_UTILIZATION_RATIO\",\n",
    "                     \"CHANGE_IN_SPENDING_BEHAVIOR\",\"TRANSACTION_VELOCITY\",\"AGE\",\"SUSPICIOUS_FLAG\",\"PREVIOUS_FRAUD_REPORTS\"]\n",
    "LABEL_COLUMNS = [\"FRAUD_INDICATOR\"]\n",
    "DROPPED_COLUMNS = [\"TRANSACTION_ID\", \"CUSTOMER_ID\", \"DOB\", \"NAME\", \"CITY\",\"COUNTY\",\"STATE\",\"YEAR\",\"QUARTER\",\"MONTH\",\"AGE_GROUP\",\"TRANSACTION_DATE\",\"ACCOUNT_OPENING_DATE\",\"LAST_LOGIN\",\"TIME_OF_DAY\",\"DAY_OF_WEEK\",\"TIME\",\"TIME_SINCE_LAST_TRANSACTION\",\"FRAUD_FLAG\"]\n",
    "OUTPUT_COLUMNS = [\"PREDICTION\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605c29d6-1048-403d-aed4-09372dad4734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter feature columns\n",
    "feature_columns = CATEGORICAL_COLUMNS + NUMERICAL_COLUMNS\n",
    "feature_columns = [col for col in feature_columns if col in Original_df.columns]\n",
    "LABEL_COLUMNS = [col for col in LABEL_COLUMNS if col in Original_df.columns]\n",
    " \n",
    "# Split data into features and labels\n",
    "X = Original_df[feature_columns + DROPPED_COLUMNS]\n",
    "y = Original_df[LABEL_COLUMNS].values.ravel()  # Flatten to 1D array for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad947e0c-7c49-41e6-85dd-16f69670c79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for numerical data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "numerical_transformer = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bb024f-08d2-4fea-b8d8-9213b15e9dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for categorical data\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ccc0e4-4b6e-458a-8078-04e304427d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, NUMERICAL_COLUMNS),\n",
    "        ('cat', categorical_transformer, CATEGORICAL_COLUMNS)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81567a26-9b94-44f0-98b5-c42f3e019777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the logistic regression model within a pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                        ('classifier', LogisticRegression())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f081268-6ae3-4cd4-b41c-53f15be74d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into training set and test set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8513756f-1ff3-4997-9f75-26375f8c42d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d798de25-942c-4a19-bd4b-5dc0eeb84957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18386af-07e7-4619-ab3d-ddf13ca3e885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your DataFrame is named 'df' and has a column 'FraudIndicator'\n",
    "# Create a count plot for the 'FraudIndicator' column\n",
    "plt.figure(figsize=(8, 6))  # Optional: Adjust the figure size\n",
    "sns.countplot(data=Original_df, x='FRAUD_INDICATOR', palette='viridis')\n",
    "plt.title('Count Plot of Fraud Indicator')\n",
    "plt.xlabel('Fraud Indicator')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9901cd0a-ff3a-4190-a3bd-05f9e76e390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9a62d1-7fcb-409c-b615-53ca6a19751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Assuming your DataFrame is named 'df' and has been preprocessed\n",
    "# Split data into features and labels\n",
    "X = Original_df[feature_columns]\n",
    "y = Original_df[LABEL_COLUMNS].values.ravel()  # Flatten to 1D array for consistency\n",
    "\n",
    "# Apply preprocessing to the data\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Initialize SMOTE for oversampling\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Apply SMOTE to the data\n",
    "X_resampled, y_resampled = smote.fit_resample(X_preprocessed, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac60d6c-e9b8-4c62-8828-1e823b2e002f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class distribution after oversampling:\", Counter(y_resampled))\n",
    "\n",
    "# Create a count plot for the 'FRAUD_INDICATOR' column after oversampling\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(data=pd.DataFrame({'FRAUD_INDICATOR': y_resampled}), x='FRAUD_INDICATOR', palette='viridis')\n",
    "plt.title('Count Plot of Fraud Indicator (After Oversampling)')\n",
    "plt.xlabel('Fraud Indicator')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43020e84-bb6b-47e9-8556-e1febfb248b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Retraining Logistic regression using SAMPLED Data\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Apply preprocessing to the testing data\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = model.predict(X_test_preprocessed)\n",
    "\n",
    "# Calculate and print various metrics to evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Model Evaluation Metrics:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ded756-f839-4b9b-809d-0761b02e7f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Define a range of hyperparameters to search\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2'],  # Regularization type\n",
    "    'C': np.logspace(-3, 3, 7),  # Inverse of regularization strength (smaller values for stronger regularization)\n",
    "    'solver': ['liblinear'],  # Solver for l1 regularization\n",
    "}\n",
    "\n",
    "# Create a grid search with cross-validation\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Get the best hyperparameters and corresponding model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Evaluate the best model on the resampled data\n",
    "y_pred = best_model.predict(X_resampled)\n",
    "\n",
    "# Calculate and print various metrics to evaluate the model's performance on the resampled data\n",
    "accuracy = accuracy_score(y_resampled, y_pred)\n",
    "precision = precision_score(y_resampled, y_pred)\n",
    "recall = recall_score(y_resampled, y_pred)\n",
    "f1 = f1_score(y_resampled, y_pred)\n",
    "confusion = confusion_matrix(y_resampled, y_pred)\n",
    "\n",
    "print(\"Model Evaluation Metrics on Resampled Data:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ddc941-8bdb-4a0a-b3d1-7d640475f3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the Random Forest Classifier model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define a range of hyperparameters to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],  # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20, 30],  # Maximum depth of the trees\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Create a grid search with cross-validation\n",
    "grid_search = GridSearchCV(rf_model, param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the resampled data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and corresponding model\n",
    "best_params = grid_search.best_params_\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train the best model on the training data\n",
    "best_rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "# Calculate and print various metrics to evaluate the best model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Best Model Evaluation Metrics:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1870c1bc-17f0-41a3-b148-71b67912334b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
