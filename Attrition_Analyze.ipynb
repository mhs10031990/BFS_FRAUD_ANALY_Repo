{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f8bbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install fosforml numpy pandas matplotlib scikit-learn seaborn python-dateutil\n",
    "!pip uninstall urllib3 -y\n",
    "!pip install urllib3==1.26.15\n",
    "!pip install fosforml \n",
    "!pip install fosforio\n",
    "!pip install refractio\n",
    "!pip install refractml\n",
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc44edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn scipy xgboost pandas dice-ml tabulate numpy scikit-learn pandas-profiling plotly matplotlib scipy statsmodels seaborn pydantic-settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b12a462",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cloudpickle' has no attribute 'list_registry_pickle_by_value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfosforml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_manager\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msnowflakesession\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_session\n\u001b[1;32m      3\u001b[0m my_session \u001b[38;5;241m=\u001b[39m get_session()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/fosforml/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_model\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m scoring_func\n\u001b[1;32m      7\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregister_model\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m ]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/fosforml/api.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelRegistry\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_manager\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msnowflakesession\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_connection_params\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfosforml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_model_name, validate_datasets_for_sf, validate_privileges\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/fosforml/model_manager/__init__.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_registry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelRegistry\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetManager, Metadata\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_metrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Classification, Regression\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelObject\n\u001b[1;32m      7\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnowflakesession\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModelRegistry\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModelObject\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     15\u001b[0m ]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/fosforml/model_manager/model_metrics.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msnowflake\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (confusion_matrix,\n\u001b[1;32m      2\u001b[0m                                         accuracy_score,\n\u001b[1;32m      3\u001b[0m                                         f1_score, recall_score,\n\u001b[1;32m      4\u001b[0m                                         precision_score,\n\u001b[1;32m      5\u001b[0m                                         log_loss ,\n\u001b[1;32m      6\u001b[0m                                         roc_auc_score ,\n\u001b[1;32m      7\u001b[0m                                         roc_curve)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfosforml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UserConfigs\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/snowflake/ml/modeling/metrics/__init__.py:14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m exportable_functions\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mglobals\u001b[39m()[k] \u001b[38;5;241m=\u001b[39m v\n\u001b[0;32m---> 14\u001b[0m registered_modules \u001b[38;5;241m=\u001b[39m \u001b[43mcloudpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_registry_pickle_by_value\u001b[49m()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m registered_modules:\n\u001b[1;32m     16\u001b[0m     cloudpickle\u001b[38;5;241m.\u001b[39mregister_pickle_by_value(result)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'cloudpickle' has no attribute 'list_registry_pickle_by_value'"
     ]
    }
   ],
   "source": [
    "from fosforml.model_manager.snowflakesession import get_session\n",
    "\n",
    "my_session = get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970ff62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = 'ATTRITION_MASTER_TABLE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6b7fd4-0b7f-4aad-a28d-fc8a5a4c7baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_df = my_session.sql(\"select * from {}\".format(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7401db-69c5-44df-847b-1f92b3c283f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pandas_df = sf_df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9e70be-c327-4700-a155-ec726c86e9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b237b4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pandas_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c3b582",
   "metadata": {},
   "outputs": [],
   "source": [
    "Original_df = pandas_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568f155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Original_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd5d514",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Original_df.drop([\"USER_ID\", \"EMPLOYEE_ID\", \"JOB_STARTDATE\", \"JOB_ENDDATE\", \"SCHOOL_ENDDATE\",\"CHURN_OTHER\",\"PEOPLE_JOINED_BEFORE_AND_LEFT_IN_THIS_MONTH\",\"PEOPLE_JOINED_AND_NEVER_LEFT\",\"TOTAL_EMPLOYEE\",\"RETENTION\",\"SUM_OF_TENURE\",\"SUM_OF_AGE\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79df4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f28381",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = [\"ROLE\",\"ETHNICITY\",\"ORGANIZATION_TYPE\", \"ORGANIZATION_OWNERSHIP\",\"COMPANY_NAME\",\"CITY\",\"STATE\",\"DISTANCE\",\"COUNTRY\",\"GENDER\",\n",
    "                       \"BUSINESS_TRAVEL\",\"ENVIRONMENT_SATISFACTION\",\"JOB_SATISFACTION\",\"MARITAL_STATUS\",\"OVER_TIME\",\"PERFORMANCE_RATING\",\"RELATIONSHIP_SATISFACTION\",\"WORK_LIFE_BALANCE\",\"CHURN_STATUS_TABLE\",\"DEGREE_CLEAN\"]\n",
    "NUMERICAL_COLUMNS = [\"SALARY\", \"SENIORITY\", \"TENURE_MONTHS\", \"MONTHS_AFTER_COLLEGE\", \"BIRTH_YEAR\",\"AGE\",\"OVERTIME_HOURS\",\"PERCENTAGE_SALARY_HIKE\"]\n",
    "LABEL_COLUMNS = [\"CHURN_VALUE_TABLE\"]\n",
    "DROPPED_COLUMNS = [\"USER_ID\", \"EMPLOYEE_ID\", \"JOB_STARTDATE\", \"JOB_ENDDATE\", \"SCHOOL_ENDDATE\",\"CHURN_OTHER\",\"PEOPLE_JOINED_BEFORE_AND_LEFT_IN_THIS_MONTH\",\"PEOPLE_JOINED_AND_NEVER_LEFT\",\"TOTAL_EMPLOYEE\",\"RETENTION\",\"SUM_OF_TENURE\",\"SUM_OF_AGE\"]\n",
    "OUTPUT_COLUMNS = [\"PREDICTION\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7722e330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter feature columns\n",
    "feature_columns = CATEGORICAL_COLUMNS + NUMERICAL_COLUMNS\n",
    "feature_columns = [col for col in feature_columns if col in Original_df.columns]\n",
    "LABEL_COLUMNS = [col for col in LABEL_COLUMNS if col in Original_df.columns]\n",
    " \n",
    "# Split data into features and labels\n",
    "X = Original_df[feature_columns + DROPPED_COLUMNS]\n",
    "y = Original_df[LABEL_COLUMNS].values.ravel()  # Flatten to 1D array for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e463892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test = train_test_split(pandas_df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3befe3-dc02-4f24-b195-0ef9015c32ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eb2566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    " \n",
    "# Define transformers\n",
    "categorical_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy='constant', fill_value='missing'),\n",
    "    OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    ")\n",
    " \n",
    "numerical_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    MinMaxScaler(clip=True)\n",
    ")\n",
    " \n",
    "# Create preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, CATEGORICAL_COLUMNS),\n",
    "        ('num', numerical_transformer, NUMERICAL_COLUMNS)\n",
    "    ]\n",
    ")\n",
    " \n",
    "# Create pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "result = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31bc983",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f24f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "filename = \"Attrition.joblib\"\n",
    "dump(pipeline, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c20f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)\n",
    "y_prob = pipeline.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a37c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from refractml import *\n",
    "\n",
    "from refractml.constants import MLModelFlavours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3b60fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@scoring_func\n",
    "def score(model, request):\n",
    "    payload_dict = request.json[\"payload\"]\n",
    "    data_json = eval(payload)\n",
    "    data = pd.DataFrame([data_json])\n",
    "    prediction = str(model.predict(data)[0])\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b764b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "import pandas as pd \n",
    "payload = str(X_test.iloc[123].to_dict())\n",
    "req = requests.Request()\n",
    "req.json = {\"payload\": payload}\n",
    "\n",
    "print(score(pipeline, req))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5169f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "req.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed7afb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## registering the model.\n",
    "from fosforml import register_model\n",
    "\n",
    "register_model(\n",
    "  model_obj=model,\n",
    "  session=my_session,\n",
    "  x_train=x_train,\n",
    "  y_train=y_train,\n",
    "  x_test=x_test,\n",
    "  y_test=y_test,\n",
    "  y_pred=y_pred,\n",
    "  source=\"Notebook\",\n",
    "  dataset_name=\"ATTRITION_MASTER_TABLE\",\n",
    "  dataset_source=\"Snowflake\",\n",
    "  name=\"attrition_master_table\",\n",
    "  description=\"This is a Model for the analyzing attrition\",\n",
    "  flavour=\"sklearn\",\n",
    "  model_type=\"classification\",\n",
    "  conda_dependencies=[\"scikit-learn==1.3.2\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff88e102",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload  = {\"payload\": X_test.iloc[0].to_dict()}\n",
    "payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971ef3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipeline.predict(X_test)\n",
    "result_prob = pipeline.predict_proba(X_test)\n",
    "pred_df = X_test.copy()\n",
    "result = result\n",
    "result_prob = result_prob\n",
    "pred_df[\"PREDICTION\"] = result\n",
    "pred_df[\"PROB\"] = result_prob[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1df31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, log_loss, roc_auc_score\n",
    " \n",
    "# Check lengths\n",
    "print(\"Length of y_test:\", len(y_test))\n",
    "print(\"Length of y_pred:\", len(y_pred))\n",
    " \n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    " \n",
    "# Calculate accuracy\n",
    "accuracy = sum(y_test == y_pred) / len(y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    " \n",
    "# Calculate additional metrics\n",
    "log_loss_value = log_loss(y_test, y_prob)\n",
    "roc_auc = roc_auc_score(y_test, y_prob[:, 1])  # Assuming class 1 is the positive class\n",
    " \n",
    "print(\"Log Loss:\", log_loss_value)\n",
    "print(\"ROC AUC Score:\", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c775c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, precision_recall_curve, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "lr_auc = roc_auc_score(y_test, pred_df[\"PROB\"])\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, pred_df[\"PROB\"])\n",
    "# plot the roc curve for the model\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "plt.plot(lr_fpr, lr_tpr, marker='.', label='Radnomforest')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f661cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_precision, lr_recall, _ = precision_recall_curve(y_test, pred_df[\"PROB\"])\n",
    "plt.plot(lr_recall, lr_precision, marker='.', label='Randomforest')\n",
    "# axis labels\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ac872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test,y_pred)\n",
    " \n",
    "#Plot the confusion matrix.\n",
    "sns.heatmap(cm,\n",
    "            annot=True,\n",
    "            fmt='g',\n",
    "            xticklabels=['Not Churn','Churn'],\n",
    "            yticklabels=['Not CHurn','Churn'])\n",
    "plt.ylabel('Prediction',fontsize=13)\n",
    "plt.xlabel('Actual',fontsize=13)\n",
    "plt.title('Confusion Matrix',fontsize=17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a50ffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_data = pd.DataFrame({\"ns_probs\":ns_probs,\n",
    "                            \"y_test\":y_test,\n",
    "                            \"y_pred\":pred_df[\"PREDICTION\"],\n",
    "                            \"act_probs\":pred_df[\"PROB\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf1f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3ca252",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_data.to_csv(\"/data/scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5599fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define or use the prediction function\n",
    "def model_prediction_score_func(dataframe):\n",
    "    # Ensure 'dataframe' has the correct features required by the model\n",
    "    return pipeline.predict(dataframe)  # Use your trained pipeline/model here\n",
    " \n",
    "# Assuming df is your input DataFrame with the necessary features\n",
    "Original_df['Model_Output'] = model_prediction_score_func(Original_df)\n",
    " \n",
    "# If you have a DataFrame with test data and you want to merge predictions\n",
    "# Assuming X_test is the DataFrame for which predictions are made\n",
    "#X_test_with_predictions = X_test.copy()\n",
    "#X_test_with_predictions['Model_Output'] = model_prediction_score_func(X_test)\n",
    " \n",
    "# Display the first few rows to verify\n",
    "#print(X_test_with_predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e079418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df0cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Original_df.to_csv(\"/data/Hr_Attrition_Data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5c08fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = snowflake.get_connection(\"HR_ATTRITION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a59f95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "if 'Model_Output' in Original_df:\n",
    "    Original_df['Model_Output_Values'] = Original_df['Model_Output'].astype(bool)\n",
    "print(Original_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9afa5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'HR_Attrition_Data.csv'\n",
    "Original_df.to_csv(file , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6656cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec1c1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowflake.execute_query(f\"PUT file://{file} @HR_ANALYTIC.PUBLIC.HR_ATTRITION_STAGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e3bf39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
